# **å¾®è°ƒ SpeechT5** 

ç°åœ¨ï¼Œæ‚¨å·²ç»ç†Ÿæ‚‰äº†æ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡å’Œ SpeechT5 æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œè¯¥æ¨¡å‹æ˜¯åœ¨è‹±è¯­æ•°æ®ä¸Šé¢„å…ˆè®­ç»ƒå¥½çš„ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†å®ƒå¾®è°ƒåˆ°å¦ä¸€ç§è¯­è¨€ã€‚

## **å†…éƒ¨ç®¡ç†** 

å¦‚æœæ‚¨æƒ³é‡ç°è¿™ä¸ªä¾‹å­ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰ GPUã€‚åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ£€æŸ¥ï¼š

```
nvidia-smi
```

> åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¤§çº¦ 40 å°æ—¶çš„è®­ç»ƒæ•°æ®ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨ Google Colab å…è´¹ GPU è·Ÿéšï¼Œåˆ™éœ€è¦å°†è®­ç»ƒæ•°æ®é‡å‡å°‘åˆ°å¤§çº¦ 10-15 å°æ—¶ï¼Œå¹¶å‡å°‘è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚

æ‚¨è¿˜éœ€è¦ä¸€äº›é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```
pip install transformers datasets soundfile speechbrain accelerate
```

æœ€åï¼Œåˆ«å¿˜äº†ç™»å½•ä½ çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·ä½ å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹äº†ï¼š

```python
from huggingface_hub import notebook_login 

notebook_login()
```

## **æ•°æ®é›†** 

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) æ•°æ®é›†çš„è·å…°è¯­`(nl`) è¯­è¨€å­é›†ã€‚[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) æ˜¯ä¸€ä¸ªå¤§å‹å¤šè¯­è¨€è¯­éŸ³è¯­æ–™åº“ï¼Œå…¶æ•°æ®æ¥æºäº 2009-2020 å¹´æ¬§æ´²è®®ä¼šæ´»åŠ¨å½•éŸ³ã€‚å®ƒåŒ…å« 15 ç§æ¬§æ´²è¯­è¨€çš„æ ‡æ³¨éŸ³é¢‘è½¬å½•æ•°æ®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è·å…°è¯­å­é›†ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–å­é›†ã€‚

è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ï¼Œå› æ­¤ï¼Œå¦‚å‰æ‰€è¿°ï¼Œå®ƒä¸æ˜¯è®­ç»ƒ TTS æ¨¡å‹çš„æœ€åˆé€‚é€‰æ‹©ã€‚ä¸è¿‡ï¼Œå¯¹äºæœ¬ç»ƒä¹ æ¥è¯´ï¼Œå®ƒå·²ç»è¶³å¤Ÿå¥½äº†ã€‚

è®©æˆ‘ä»¬åŠ è½½æ•°æ®ï¼š

```python
from datasets import load_dataset, Audio 

dataset = load_dataset("facebook/voxpopuli", "nl", split="train") 
len(dataset)
```

**è¾“å‡ºï¼š**

```
20968
```

20968 ä¸ªç¤ºä¾‹è¶³ä»¥è¿›è¡Œå¾®è°ƒã€‚SpeechT5 å¸Œæœ›éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡ä¸º 16 kHzï¼Œå› æ­¤è¦ç¡®ä¿æ•°æ®é›†ä¸­çš„ç¤ºä¾‹ç¬¦åˆè¿™ä¸€è¦æ±‚ï¼š

```python
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

## **é¢„å¤„ç†æ•°æ®** 

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰è¦ä½¿ç”¨çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¹¶åŠ è½½é€‚å½“çš„å¤„ç†å™¨ï¼Œè¯¥å¤„ç†å™¨åŒ…å« tokenizer å’Œç‰¹å¾æå–å™¨ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬æ¥å‡†å¤‡è®­ç»ƒæ•°æ®ï¼š

```python
from transformers import SpeechT5Processor 

checkpoint = "microsoft/speecht5_tts" 
processor = SpeechT5Processor.from_pretrained(checkpoint)
```

### **ä¸º SpeechT5 tokenization æ¸…ç†æ–‡æœ¬** 

é¦–å…ˆï¼Œä¸ºäº†å‡†å¤‡æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†å™¨çš„ tokenizer éƒ¨åˆ†ï¼Œè®©æˆ‘ä»¬æ¥è·å–å®ƒï¼š

```python
tokenizer = processor.tokenizer
```

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```python
dataset[0]
```

**è¾“å‡ºï¼š**

```
{'audio_id': '20100210-0900-PLENARY-3-nl_20100210-09:06:43_4', 
 'language': 9, 
 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/02ec6a19d5b97c03e1379250378454dbf3fa2972943504a91c7da5045aa26a89/train_part_0/20100210-0900-PLENARY-3-nl_20100210-09:06:43_4.wav', 
 'array': array([ 4.27246094e-04, 1.31225586e-03, 1.03759766e-03, . ...ï¼Œ-9.15527344e-05ï¼Œ7.62939453e-04ï¼Œ-2.44140625e-04]), 'sampling_rate': 16000}, 
 'raw_text': 'Dat kan naar mijn gevoel alleen met een brede meerderheid die wijen sameken zoeken. 'ï¼Œ
 'normalized_text'ï¼š'dat kan naar myn gevoel alleen met een brede meerderheid die wij samen zoeken.'ï¼Œ
 'gender'ï¼š'female'ï¼Œ
 'speaker_id'ï¼š'1122'ï¼Œ
 'is_gold_transcript'ï¼š True, 
 'accent': 'None'}
```

æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œæ•°æ®é›†ç¤ºä¾‹åŒ…å«åŸå§‹æ–‡æœ¬ï¼ˆ`raw_text`ï¼‰å’Œæ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆ`normalized_text`ï¼‰ç‰¹å¾ã€‚åœ¨å†³å®šä½¿ç”¨å“ªç§ç‰¹å¾ä½œä¸ºæ–‡æœ¬è¾“å…¥æ—¶ï¼Œé‡è¦çš„æ˜¯è¦çŸ¥é“ SpeechT5 tokenizer æ²¡æœ‰ä»»ä½•æ•°å­—æ ‡è®°ã€‚åœ¨æ ‡å‡†åŒ–æ–‡æœ¬ (`normalized_text`) ä¸­ï¼Œæ•°å­—è¢«å†™æˆæ–‡æœ¬ã€‚å› æ­¤ï¼Œå®ƒæ›´é€‚åˆï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ `normalized_text` ä½œä¸ºè¾“å…¥æ–‡æœ¬ã€‚

ç”±äº SpeechT5 æ˜¯åœ¨è‹±è¯­è¯­è¨€ç¯å¢ƒä¸­è®­ç»ƒçš„ï¼Œå› æ­¤å®ƒå¯èƒ½æ— æ³•è¯†åˆ«è·å…°è¯­æ•°æ®é›†ä¸­çš„æŸäº›å­—ç¬¦ã€‚å¦‚æœä¿æŒåŸæ ·ï¼Œè¿™äº›å­—ç¬¦å°†è¢«è½¬æ¢ä¸º`<unk>`æ ‡è®°ã€‚ä½†æ˜¯ï¼Œåœ¨è·å…°è¯­ä¸­ï¼ŒæŸäº›å­—ç¬¦ï¼ˆå¦‚`Ã `ï¼‰ç”¨äºå¼ºè°ƒéŸ³èŠ‚ã€‚ä¸ºäº†ä¿ç•™æ–‡æœ¬çš„æ„æ€ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ™®é€šçš„ `a` ä»£æ›¿è¿™ä¸ªå­—ç¬¦ã€‚

è¦è¯†åˆ«ä¸æ”¯æŒçš„æ ‡è®°ï¼Œå¯ä½¿ç”¨ `SpeechT5Tokenizer` æå–æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç‹¬ç‰¹å­—ç¬¦ï¼Œè¯¥å·¥å…·å¯å°†å­—ç¬¦ä½œä¸ºæ ‡è®°ä½¿ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ç¼–å†™ `extract_all_chars` æ˜ å°„å‡½æ•°ï¼Œå°†æ‰€æœ‰ç¤ºä¾‹çš„è½¬å½•å¹¶å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ç»„å­—ç¬¦ã€‚è¯·ç¡®ä¿åœ¨ `dataset.map()` ä¸­è®¾ç½® `batched=True` å’Œ `batch_size=-1`ï¼Œä»¥ä¾¿æ˜ å°„å‡½æ•°å¯ä»¥ä¸€æ¬¡æ€§ä½¿ç”¨æ‰€æœ‰è½¬å½•ã€‚

```python
def extract_all_chars(batch):
    all_text = " ".join(batch["normalized_text"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}


vocabs = dataset.map(
    extract_all_chars,
    batched=True,
    batch_size=-1,
    keep_in_memory=True,
    remove_columns=dataset.column_names,
)

dataset_vocab = set(vocabs["vocab"][0])
tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}
```

ç°åœ¨æœ‰ä¸¤ç»„å­—ç¬¦ï¼šä¸€ç»„æ˜¯æ•°æ®é›†ä¸­çš„è¯æ±‡ï¼Œå¦ä¸€ç»„æ˜¯ tokenizer ä¸­çš„è¯æ±‡ã€‚è¦è¯†åˆ«æ•°æ®é›†ä¸­ä»»ä½•ä¸æ”¯æŒçš„å­—ç¬¦ï¼Œå¯ä»¥å–è¿™ä¸¤ç»„å­—ç¬¦çš„å·®å€¼ã€‚å¾—åˆ°çš„å­—ç¬¦é›†å°†åŒ…å«æ•°æ®é›†ä¸­çš„å­—ç¬¦ï¼Œä½†ä¸åŒ…å« tokenizer ä¸­çš„å­—ç¬¦ã€‚

```
dataset_vocab - tokenizer_vocab
```

**è¾“å‡ºï¼š**

```
{' ', 'Ã ', 'Ã§', 'Ã¨', 'Ã«', 'Ã­', 'Ã¯', 'Ã¶', 'Ã¼'}
```

ä¸ºäº†å¤„ç†ä¸Šä¸€æ­¥ä¸­è¯†åˆ«å‡ºçš„ä¸æ”¯æŒçš„å­—ç¬¦ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†è¿™äº›å­—ç¬¦æ˜ å°„ä¸ºæœ‰æ•ˆçš„æ ‡è®°ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç©ºæ ¼å·²ç»åœ¨æ ‡è®°ç¬¦å·ç”Ÿæˆå™¨ä¸­è¢« `â–` æ›¿æ¢ï¼Œæ— éœ€å•ç‹¬å¤„ç†ã€‚

```python
replacements = [
    ("Ã ", "a"),
    ("Ã§", "c"),
    ("Ã¨", "e"),
    ("Ã«", "e"),
    ("Ã­", "i"),
    ("Ã¯", "i"),
    ("Ã¶", "o"),
    ("Ã¼", "u"),
]


def cleanup_text(inputs):
    for src, dst in replacements:
        inputs["normalized_text"] = inputs["normalized_text"].replace(src, dst)
    return inputs


dataset = dataset.map(cleanup_text)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å¤„ç†äº†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œæ˜¯æ—¶å€™å°†é‡ç‚¹è½¬ç§»åˆ°éŸ³é¢‘æ•°æ®ä¸Šäº†ã€‚

### è¯´è¯äºº 

VoxPopuli æ•°æ®é›†åŒ…å«å¤šä¸ªè¯´è¯äººçš„è¯­éŸ³ï¼Œä½†æ•°æ®é›†ä¸­æœ‰å¤šå°‘è¯´è¯äººå‘¢ï¼Ÿè¦ç¡®å®šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç‹¬ç‰¹è¯´è¯è€…çš„æ•°é‡ä»¥åŠæ¯ä¸ªè¯´è¯è€…ä¸ºæ•°æ®é›†è´¡çŒ®çš„ç¤ºä¾‹æ•°é‡ã€‚æ•°æ®é›†ä¸­æ€»å…±æœ‰ 20968 ä¸ªç¤ºä¾‹ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°äº†è§£æ•°æ®ä¸­è¯´è¯è€…å’Œç¤ºä¾‹çš„åˆ†å¸ƒæƒ…å†µã€‚

```python
from collections import defaultdict

speaker_counts = defaultdict(int)

for speaker_id in dataset["speaker_id"]:
    speaker_counts[speaker_id] += 1
```

é€šè¿‡ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œæ‚¨å¯ä»¥äº†è§£æ¯ä¸ªå‘è¨€äººçš„æ•°æ®é‡ã€‚

```python
import matplotlib.pyplot as plt

plt.figure()
plt.hist(speaker_counts.values(), bins=20)
plt.ylabel("Speakers")
plt.xlabel("Examples")
plt.show()
```

![tts_speakers_histogram](images/tts_speakers_histogram.png)ç›´æ–¹å›¾æ˜¾ç¤ºï¼Œæ•°æ®é›†ä¸­çº¦æœ‰ä¸‰åˆ†ä¹‹ä¸€çš„è¯´è¯äººçš„ä¾‹å­å°‘äº 100 ä¸ªï¼Œè€Œçº¦æœ‰ 10 ä¸ªè¯´è¯äººçš„ä¾‹å­è¶…è¿‡ 500 ä¸ªã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡å¹¶å¹³è¡¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•°æ®é™åˆ¶åœ¨ 100 åˆ° 400 ä¸ªç¤ºä¾‹ä¹‹é—´ã€‚

```python
def select_speaker(speaker_id):
    return 100 <= speaker_counts[speaker_id] <= 400


dataset = dataset.filter(select_speaker, input_columns=["speaker_id"])
```

è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹è¿˜å‰©ä¸‹å¤šå°‘å‘è¨€äººï¼š

```python
len(set(dataset["speaker_id"]))
```

**è¾“å‡ºï¼š**

```
42
```

æˆ‘ä»¬æ¥çœ‹çœ‹è¿˜å‰©ä¸‹å¤šå°‘ä¾‹å­ï¼š

```python
len(dataset)
```

**è¾“å‡ºï¼š 9973**

```
9973
```

ç°åœ¨åªå‰©ä¸‹ä¸åˆ° 10,000 ä¸ªä¾‹å­ï¼Œè¿™äº›ä¾‹å­æ¥è‡ªå¤§çº¦ 40 ä¸ªä¸åŒçš„è¯´è¯äººï¼Œåº”è¯¥è¶³å¤Ÿäº†ã€‚

è¯·æ³¨æ„ï¼Œå¦‚æœç¤ºä¾‹è¾ƒé•¿ï¼Œä¸€äº›ç¤ºä¾‹è¾ƒå°‘çš„è¯´è¯äººå®é™…ä¸Šå¯èƒ½æœ‰æ›´å¤šå¯ç”¨éŸ³é¢‘ã€‚ä¸è¿‡ï¼Œè¦ç¡®å®šæ¯ä¸ªè¯´è¯äººçš„éŸ³é¢‘æ€»é‡ï¼Œéœ€è¦å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ‰«æï¼Œè€Œè¿™æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹ï¼Œéœ€è¦å¯¹æ¯ä¸ªéŸ³é¢‘æ–‡ä»¶è¿›è¡ŒåŠ è½½å’Œè§£ç ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©è·³è¿‡è¿™ä¸€æ­¥ã€‚

### **è¯´è¯äººåµŒå…¥** 

ä¸ºä½¿ TTS æ¨¡å‹èƒ½åŒºåˆ†å¤šä¸ªè¯´è¯è€…ï¼Œæ‚¨éœ€è¦ä¸ºæ¯ä¸ªç¤ºä¾‹åˆ›å»ºè¯´è¯äººåµŒå…¥ã€‚è¯´è¯è€…åµŒå…¥æ˜¯å¯¹æ¨¡å‹çš„é¢å¤–è¾“å…¥ï¼Œå¯æ•æ‰ç‰¹å®šè¯´è¯è€…çš„è¯­éŸ³ç‰¹å¾ã€‚è¦ç”Ÿæˆè¿™äº›è¯´è¯è€…åµŒå…¥ï¼Œå¯ä½¿ç”¨ SpeechBrain æä¾›çš„é¢„è®­ç»ƒ [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb) æ¨¡å‹ã€‚

åˆ›å»ºå‡½æ•° `create_speaker_embedding()` ï¼Œè¯¥å‡½æ•°æ¥æ”¶è¾“å…¥éŸ³é¢‘æ³¢å½¢ï¼Œå¹¶è¾“å‡ºåŒ…å«ç›¸åº”æ‰¬å£°å™¨åµŒå…¥çš„ 512 å…ƒå‘é‡ã€‚

```python
import os
import torch
from speechbrain.pretrained import EncoderClassifier

spk_model_name = "speechbrain/spkrec-xvect-voxceleb"

device = "cuda" if torch.cuda.is_available() else "cpu"
speaker_model = EncoderClassifier.from_hparams(
    source=spk_model_name,
    run_opts={"device": device},
    savedir=os.path.join("/tmp", spk_model_name),
)


def create_speaker_embedding(waveform):
    with torch.no_grad():
        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))
        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)
        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()
    return speaker_embeddings
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ`speechbrain/spkrec-xvect-voxceleb` æ¨¡å‹æ˜¯åœ¨ VoxCeleb æ•°æ®é›†ä¸­çš„è‹±è¯­è¯­éŸ³åŸºç¡€ä¸Šè®­ç»ƒçš„ï¼Œè€Œæœ¬æŒ‡å—ä¸­çš„è®­ç»ƒç¤ºä¾‹æ˜¯è·å…°è¯­ã€‚è™½ç„¶æˆ‘ä»¬ç›¸ä¿¡è¯¥æ¨¡å‹ä»èƒ½ä¸ºæˆ‘ä»¬çš„è·å…°è¯­æ•°æ®é›†ç”Ÿæˆåˆç†çš„è¯´è¯è€…åµŒå…¥ï¼Œä½†è¿™ä¸€å‡è®¾å¹¶éåœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æˆç«‹ã€‚

ä¸ºäº†è·å¾—æœ€ä½³ç»“æœï¼Œæˆ‘ä»¬éœ€è¦å…ˆåœ¨ç›®æ ‡è¯­éŸ³ä¸Šè®­ç»ƒä¸€ä¸ª X å‘é‡æ¨¡å‹ã€‚è¿™å°†ç¡®ä¿æ¨¡å‹èƒ½æ›´å¥½åœ°æ•æ‰è·å…°è¯­ä¸­ç‹¬ç‰¹çš„è¯­éŸ³ç‰¹å¾ã€‚å¦‚æœæ‚¨æƒ³è®­ç»ƒè‡ªå·±çš„ X å‘é‡æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨[æ­¤è„šæœ¬](https://huggingface.co/mechanicalsea/speecht5-vc/blob/main/manifest/utils/prep_cmu_arctic_spkemb.py)ä½œä¸ºç¤ºä¾‹ã€‚

### **å¤„ç†æ•°æ®é›†** 

æœ€åï¼Œè®©æˆ‘ä»¬å°†æ•°æ®å¤„ç†æˆæ¨¡å‹æ‰€éœ€çš„æ ¼å¼ã€‚åˆ›å»ºä¸€ä¸ª `prepare_dataset` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶å•ä¸ªç¤ºä¾‹ï¼Œä½¿ç”¨ `SpeechT5Processor`å¯¹è±¡å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œ tokenize ï¼Œå¹¶å°†ç›®æ ‡éŸ³é¢‘åŠ è½½åˆ°å¯¹æ•°æ¢…å°”é¢‘è°±å›¾ä¸­ã€‚å®ƒè¿˜åº”æ·»åŠ è¯´è¯è€…åµŒå…¥ä½œä¸ºé¢å¤–è¾“å…¥ã€‚

```python
def prepare_dataset(example):
    audio = example["audio"]

    example = processor(
        text=example["normalized_text"],
        audio_target=audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_attention_mask=False,
    )

    # strip off the batch dimension
    example["labels"] = example["labels"][0]

    # use SpeechBrain to obtain x-vector
    example["speaker_embeddings"] = create_speaker_embedding(audio["array"])

    return example
```

æŸ¥çœ‹å•ä¸ªç¤ºä¾‹ï¼ŒéªŒè¯å¤„ç†æ˜¯å¦æ­£ç¡®ï¼š

```python
processed_example = prepare_dataset(dataset[0])
list(processed_example.keys())
```

**è¾“å‡ºï¼š**

```
['input_ids','labels','stop_labels','speaker_embeddings']ã€‚
```

è¯´è¯äººåµŒå…¥åº”è¯¥æ˜¯ä¸€ä¸ª 512 å…ƒç´ çš„å‘é‡ï¼š

```python
processed_example["speaker_embeddings"].shape
```

**è¾“å‡ºï¼š**

```
(512,)
```

æ ‡ç­¾åº”è¯¥æ˜¯æœ‰ 80 ä¸ªæ¢…å°” bins çš„å¯¹æ•°æ¢…å°”é¢‘è°±å›¾ã€‚

```python
import matplotlib.pyplot as plt

plt.figure()
plt.imshow(processed_example["labels"].T)
plt.show()
```

![tts_logmelspectrogram_1](images/tts_logmelspectrogram_1.png)

é¢˜å¤–è¯ï¼šå¦‚æœæ‚¨è§‰å¾—è¿™å¼ é¢‘è°±å›¾ä»¤äººå›°æƒ‘ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‚¨ç†Ÿæ‚‰å°†ä½é¢‘æ”¾åœ¨å›¾è¡¨åº•éƒ¨ã€é«˜é¢‘æ”¾åœ¨å›¾è¡¨é¡¶éƒ¨çš„æƒ¯ä¾‹ã€‚ä¸è¿‡ï¼Œåœ¨ä½¿ç”¨ matplotlib åº“å°†é¢‘è°±å›¾ç»˜åˆ¶æˆå›¾åƒæ—¶ï¼ŒY è½´ä¼šç¿»è½¬ï¼Œé¢‘è°±å›¾ä¹Ÿä¼šå€’ç½®ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨å¤„ç†å‡½æ•°ã€‚è¿™éœ€è¦ 5 åˆ° 10 åˆ†é’Ÿã€‚

```python
dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)
```

æ‚¨ä¼šçœ‹åˆ°ä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºæ•°æ®é›†ä¸­çš„æŸäº›ç¤ºä¾‹é•¿åº¦è¶…è¿‡äº†æ¨¡å‹å¯å¤„ç†çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆ600 ä¸ª tokenï¼‰ã€‚è¯·ä»æ•°æ®é›†ä¸­åˆ é™¤è¿™äº›ç¤ºä¾‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¼šæ›´è¿›ä¸€æ­¥ï¼Œä¸ºäº†é€‚åº”æ›´å¤§çš„æ‰¹é‡ï¼Œæˆ‘ä»¬ä¼šåˆ é™¤è¶…è¿‡ 200 ä¸ªè¯ç»„çš„å†…å®¹ã€‚

```python
def is_not_too_long(input_ids):
    input_length = len(input_ids)
    return input_length < 200


dataset = dataset.filter(is_not_too_long, input_columns=["input_ids"])
len(dataset)
```

**è¾“å‡ºï¼š**

```
8259
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºåŸºæœ¬çš„è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ï¼š

```python
dataset = dataset.train_test_split(test_size=0.1)
```

### **æ•°æ®æ•´ç†å™¨** 

ä¸ºäº†å°†å¤šä¸ªç¤ºä¾‹åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œä½ éœ€è¦å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ã€‚è¯¥æ•´ç†å™¨å°†ç”¨å¡«å……æ ‡è®°å¡«å……è¾ƒçŸ­çš„åºåˆ—ï¼Œç¡®ä¿æ‰€æœ‰ç¤ºä¾‹å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚å¯¹äºé¢‘è°±å›¾æ ‡ç­¾ï¼Œå¡«å……çš„éƒ¨åˆ†ä¼šç”¨ç‰¹æ®Šå€¼ `-100`  ä»£æ›¿ã€‚è¿™ä¸ªç‰¹æ®Šå€¼æŒ‡ç¤ºæ¨¡å‹åœ¨è®¡ç®—é¢‘è°±å›¾æŸå¤±æ—¶å¿½ç•¥é¢‘è°±å›¾çš„è¿™ä¸€éƒ¨åˆ†ã€‚

```python
from dataclasses import dataclass
from typing import Any, Dict, List, Union


@dataclass
class TTSDataCollatorWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_ids = [{"input_ids": feature["input_ids"]} for feature in features]
        label_features = [{"input_values": feature["labels"]} for feature in features]
        speaker_features = [feature["speaker_embeddings"] for feature in features]

        # collate the inputs and targets into a batch
        batch = processor.pad(
            input_ids=input_ids, labels=label_features, return_tensors="pt"
        )

        # replace padding with -100 to ignore loss correctly
        batch["labels"] = batch["labels"].masked_fill(
            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100
        )

        # not used during fine-tuning
        del batch["decoder_attention_mask"]

        # round down target lengths to multiple of reduction factor
        if model.config.reduction_factor > 1:
            target_lengths = torch.tensor(
                [len(feature["input_values"]) for feature in label_features]
            )
            target_lengths = target_lengths.new(
                [
                    length - length % model.config.reduction_factor
                    for length in target_lengths
                ]
            )
            max_length = max(target_lengths)
            batch["labels"] = batch["labels"][:, :max_length]

        # also add in the speaker embeddings
        batch["speaker_embeddings"] = torch.tensor(speaker_features)

        return batch
```

åœ¨ SpeechT5 ä¸­ï¼Œæ¨¡å‹ä¸­è§£ç å™¨éƒ¨åˆ†çš„è¾“å…¥å‡å°‘äº† 2 å€ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒèˆå¼ƒäº†ç›®æ ‡åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªå…¶ä»–æ—¶é—´æ­¥ã€‚è§£ç å™¨é¢„æµ‹çš„åºåˆ—é•¿åº¦æ˜¯åŸæ¥çš„ä¸¤å€ã€‚ç”±äºåŸå§‹ç›®æ ‡åºåˆ—çš„é•¿åº¦å¯èƒ½æ˜¯å¥‡æ•°ï¼Œæ•°æ®æ•´ç†å™¨ä¼šç¡®ä¿å°†æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦å–æ•´ä¸º 2 çš„å€æ•°ã€‚

```python
data_collator = TTSDataCollatorWithPadding(processor=processor)
```

## **è®­ç»ƒæ¨¡å‹** 

ä»ä¸åŠ è½½å¤„ç†å™¨ç›¸åŒçš„æ£€æŸ¥ç‚¹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š

```python
from transformers import SpeechT5ForTextToSpeech

model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)
```

`use_cache=True` é€‰é¡¹ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸å…¼å®¹ã€‚åœ¨è®­ç»ƒæ—¶ç¦ç”¨å®ƒï¼Œåœ¨ç”Ÿæˆæ—¶é‡æ–°å¯ç”¨ç¼“å­˜ï¼Œä»¥åŠ å¿«æ¨ç†æ—¶é—´ï¼š

```
from functools import partial

# disable cache during training since it's incompatible with gradient checkpointing
model.config.use_cache = False

# set language and task for generation and re-enable cache
model.generate = partial(model.generate, use_cache=True)
```

å®šä¹‰è®­ç»ƒå‚æ•°ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—ä»»ä½•è¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢è®¨è®ºè¯„ä¼°ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†åªæŸ¥çœ‹æŸå¤±ï¼š

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="speecht5_finetuned_voxpopuli_nl",  # change to a repo name of your choice
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=4000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=2,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    greater_is_better=False,
    label_names=["labels"],
    push_to_hub=True,
)
```

å®ä¾‹åŒ– `Trainer` å¯¹è±¡ï¼Œå¹¶å°†æ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨ä¼ é€’ç»™å®ƒã€‚

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    data_collator=data_collator,
    tokenizer=processor,
)
```

è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼è®­ç»ƒéœ€è¦å‡ ä¸ªå°æ—¶ã€‚æ ¹æ® GPU çš„ä¸åŒï¼Œå¼€å§‹è®­ç»ƒæ—¶å¯èƒ½ä¼šé‡åˆ° CUDA "å†…å­˜ä¸è¶³ "çš„é”™è¯¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥å°†æ¯å°è®¾å¤‡çš„è®­ç»ƒæ‰¹é‡ï¼ˆ`per_device_train_batch_size`ï¼‰æŒ‰ 2 å€é€’å‡ï¼Œå¹¶å°†æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆ`gradient_accumulation_steps`ï¼‰å¢åŠ  2 å€æ¥å¼¥è¡¥ã€‚

```python
trainer.train()
```

å°†æœ€ç»ˆæ¨¡å‹æ¨é€åˆ° ğŸ¤— Hubï¼š

```python
trainer.push_too_hub()
```

## **æ¨ç†** 

å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒåï¼Œå°±å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼ä» ğŸ¤— Hub åŠ è½½æ¨¡å‹ï¼ˆç¡®ä¿åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ä½¿ç”¨æ‚¨çš„è´¦æˆ·åï¼‰ï¼š

```python
model = SpeechT5ForTextToSpeech.from_pretrained(
    "YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl"
)
```

é€‰å–ä¸€ä¸ªç¤ºä¾‹ï¼Œè¿™é‡Œæˆ‘ä»¬ä»æµ‹è¯•æ•°æ®é›†ä¸­é€‰å–ä¸€ä¸ªã€‚è·å–è¯´è¯è€…åµŒå…¥ã€‚

```python
example = dataset["test"][304]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
```

å®šä¹‰è¾“å…¥æ–‡æœ¬å¹¶ tokenize ã€‚

```python
text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
```

é¢„å¤„ç†è¾“å…¥æ–‡æœ¬

```python
inputs = processor(text=text, return_tensors="pt")
```

å®ä¾‹åŒ–å£°ç å™¨å¹¶ç”Ÿæˆè¯­éŸ³ï¼š

```python
from transformers import SpeechT5HifiGan

vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)
```

å‡†å¤‡å¥½è†å¬ç»“æœäº†å—ï¼Ÿ

```python
from IPython.display import Audio

Audio(speech.numpy(), rate=16000)
```

åœ¨ä¸€ç§æ–°è¯­è¨€ä¸Šä½¿ç”¨è¯¥æ¨¡å‹è·å¾—ä»¤äººæ»¡æ„çš„ç»“æœå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯´è¯è€…åµŒå…¥çš„è´¨é‡å¯èƒ½æ˜¯ä¸€ä¸ªé‡è¦å› ç´ ã€‚ç”±äº SpeechT5 ä½¿ç”¨è‹±è¯­ x å‘é‡è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå› æ­¤å®ƒåœ¨ä½¿ç”¨è‹±è¯­æ‰¬å£°å™¨åµŒå…¥æ—¶è¡¨ç°æœ€ä½³ã€‚å¦‚æœåˆæˆè¯­éŸ³å¬èµ·æ¥å¾ˆå·®ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ä¸åŒçš„è¯´è¯è€…åµŒå…¥ã€‚

å»¶é•¿è®­ç»ƒæ—¶é—´ä¹Ÿæœ‰å¯èƒ½æé«˜ç»“æœçš„è´¨é‡ã€‚å³ä¾¿å¦‚æ­¤ï¼Œè¯­éŸ³æ˜¾ç„¶æ˜¯è·å…°è¯­è€Œä¸æ˜¯è‹±è¯­ï¼Œè€Œä¸”å®ƒç¡®å®æ•æ‰åˆ°äº†è¯´è¯è€…çš„å£°éŸ³ç‰¹å¾ï¼ˆä¸ç¤ºä¾‹ä¸­çš„åŸå§‹éŸ³é¢‘è¿›è¡Œæ¯”è¾ƒï¼‰ã€‚å¦ä¸€ä¸ªéœ€è¦å°è¯•çš„æ˜¯æ¨¡å‹çš„é…ç½®ã€‚ä¾‹å¦‚ï¼Œå°è¯•ä½¿ç”¨`config.reduction_factor = 1`ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æ”¹å–„ç»“æœã€‚

ä¸‹ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•è¯„ä¼°æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚
